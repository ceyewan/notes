# Distributed Systems for Fun and Profit

## 1 Introduction

这本书作者写于 2013 年，所以有些内容可能有点过时，有些新技术可能没有涉及。

分布式编程大部分时候都是在处理分布的两个后果的影响：

-   信息以光速传播
-   独立的事物会独立失败

即分布式编程的核心是处理**距离**和**存在多个事物**。

本文分为五个章节，分别是基础知识（可拓展性、可用性、性能、延迟和容错等）、抽象层次的上下变化（抽象概念和不可能性结果、CAP 定理）、时间及顺序、复制：防止分歧（2PC 和 paxos）、复制：接受分歧（弱一致性）。

## 2 Distributed Systems at a High Level 分布式系统概述

分布式编程是使用多台计算机来解决可以在一台计算机上解决的同一问题的艺术。每台计算机上都要执行**存储**和**计算**两个基本任务。

如果有无限的资源，那么可以设计一台计算机完成计算，但是很少人拥有无限资源，现实中，最佳实践是使用中端的通用硬件并使用**容错技术**将维护成本保持在较低水平。

### 2.1 Scalability

> 可拓展性是系统、网络或进程以有效的方式处理不断增长的工作量的能力，或者是扩大其规模以适应这种增长的能力。

-   **规模可拓展性**：添加更多节点应该使系统线性更快 ​​；增加数据集不应增加延迟。
-   **地理可拓展性**：应该可以使用多个数据中心来减少响应用户查询所需的时间，同时以某种合理的方式处理跨数据中心的延迟。
-   **管理可拓展性**：添加更多节点不应增加系统的管理成本（例如管理员与机器的比率）。

一个可扩展的系统是指随着规模的增加仍能满足其用户需求的系统。我们需要特别关心性能和可用性这两个方面。

### 2.2 Performance（latency）

> 性能是指计算机系统完成的有用工作量与其所使用的时间和资源相比的特征。可能会涉及：
>
> -   短响应时间（低延迟）
> -   高吞吐量
> -   低的计算资源利用率

针对这些方面优化都需要权衡，例如，系统可以通过处理更大的工作批次来提高吞吐量，从而减少操作开销。权衡则是由于批处理而导致单个工作单元的响应时间变长。

延迟指的是从某事开始到发生之间的一段时期，即是事件发生与产生可见影响之间的时间。在分布式系统中，存在一个无法克服的最小延迟：光速限制了信息传播的速度，硬件执行一次操作都会产生最小的延迟成本。

### 2.3 Availability（fault tolerance）

> 可用性是系统处于正常运行状态的时间比例。如果用户无法访问系统，则称该系统不可用。分布式系统采用一堆不可靠的组件，并在它们之上构建一个**可靠**的系统。主要是通过**容错技术**来保证的。

### 2.4 What Prevents Us

分布式系统受到两个物理因素的约束：

-   节点数量（随着所需存储和计算能力的增加而增加）
-   节点之间的距离（信息最多以光速传播）

独立节点数量的增加会增加系统发生故障的可能性（降低可用性并增加管理成本）；

独立节点数量的增加会增加节点之间通信的需求（随着规模的增加而降低性能）；

地理距离的增加会增加远程节点之间通信的最小延迟（降低某些操作的性能）。

### 2.5 Abstractions and Models

抽象通过消除与解决问题无关的现实世界方面使事情更易于管理。模型以精确的方式描述分布式系统的关键属性。下一章将讨论许多类型的模型：

-   系统模型（异步、同步）
-   故障模型（崩溃-故障、分区、拜占庭）
-   一致性模型（强一致性、最终一致性）

### 2.6 Design techniques：partition and Replicate 分区与复制

抽象通过消除与解决问题无关的现实世界方面使事情更易于管理。模型以精确的方式描述分布式系统的关键属性。

有两种可应用于数据集的基本技术。它可以分为多个节点（**分区**）以允许更多并行处理。它还可以复制或缓存在不同的节点上，以减少客户端和服务器之间的距离并提高容错能力（**复制**）。下图中的 A 和 B 被划分为独立的集合，而 C 被复制到多个位置。

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250212132639.png)

#### 2.6.1 分区

分区是将数据集划分为较小的不同独立集合以减少数据集增长的影响，因为每个分区都是数据的子集。

-   分区通过限制要处理的数据量并将相关数据定位在同一分区中来提高性能。
-   分区通过允许分区独立故障来提高可用性，增加在牺牲可用性之前需要故障的节点数量。

#### 2.6.2 复制

复制是在多台机器上制作相同数据的副本；这允许更多的服务器参与计算。是抵抗延迟的主要方式。

-   **提高性能**：通过在多台机器上复制相同的数据，允许更多的服务器参与计算，从而减少延迟。
-   **增强可用性**：复制通过创建数据的额外副本提高了可用性，增加了在牺牲可用性之前需要失败的节点数量。

复制也带来了新的挑战。

-   **同步问题**：多个独立的副本需要保持一致性，确保数据在不同机器上同步。
-   **一致性模型的选择**：一致性模型决定了如何保证数据的同步与一致性，并影响系统的可用性与延迟。

> **强一致性**指的是在分布式系统中，所有节点的状态在任何时刻都是一致的。当一个写操作完成后，所有后续的读操作都能看到这个更新，且不会返回过时的数据。它通常通过锁或同步机制来确保，保证了数据的一致性，但代价是可能会引入较高的延迟。强一致性确保了数据的准确性，但在网络故障或高延迟环境下可能会影响系统的可用性。
> **弱一致性**则表示系统在某些时间点可能存在不同步的状态，允许节点之间的数据在短时间内处于不一致的状态。即便某个节点的更新没有被立即同步到其他节点，系统仍然认为是正常的，最终一致性通常在这种情况下得到保证。弱一致性强调高可用性和性能，通常适用于对一致性要求不高、容忍暂时数据不一致的场景，如社交媒体应用、缓存系统等。

## 3 Up and down the Level of Abstraction

在本章中，我们将上下探索抽象层次，查看一些不可能性结果（CAP 和 FLP），然后为了性能再返回较低层次。

抽象本质上是虚假的，因为每种情况都是独一无二的，但是抽象使得世界变得可以管理。

### 3.1 A System Model

分布式系统的一个关键特性是分布式，即分布式系统中的程序：

-   在独立节点上并行的运行
-   通过网络连接（连接可能引入非确定性和消息丢失）
-   没有共享内存或共享时钟

> **系统模型**：关于分布式系统所实现的环境和设施的一组假设。

一个健壮的系统模型是那种做出最弱假设的模型。另一方面，也可以做出强有力的假设来创建一个易于推理的系统模型。

### 3.2 Nodes

节点是计算和存储的主机，具有：

-   执行程序的能力
-   将数据存储到易失性内存和稳定状态的能力（内存/硬盘）
-   一个时钟（可能准确也可能不准确）

节点执行**确定性算法**：局部计算、计算后的局部状态以及发送的消息唯一地由**接收到的消息**和**接收消息时的局部状态**决定。

大多数系统假设的故障模型都是**崩溃故障模型**，即：**节点只能通过崩溃来发生故障，并且可能在稍后某个时间点恢复。**

> **拜占庭容错**：假设节点可以通过任意方式的不当行为来失败，也就是说节点除了会失效，也有可能变成恶意节点。

### 3.3 Communication Links

通信链路将各个节点相互连接，并允许消息在两个方向上传送。许多讨论分布式算法的书籍假设每个节点对之间存在单独的链路，这些链路为消息提供 FIFO（先入先出）顺序，只能传递已发送的消息，并且发送的消息可能会丢失。但一般来说，最好**将网络视为不可靠的，并且容易出现消息丢失和延迟。**

**网络分区**：节点还在运行但是网络发生了故障，这种情况下，消息坑你会丢失或延迟，但是每个分区的节点可能对于某些客户端还是可以访问的，因此必须与**崩溃**的节点区别对待。

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250212134851.png)

### 3.4 Timing / Ordering Assumptions

物理分布的一个后果是每个节点以独特的方式体验世界，因为信息只能以光速传播，如果节点之间的距离不同，那么消息就是会以不同的时间到达。

> **同步系统模型**：进程按步调一致地执行；消息传输延迟有一个已知的上限；每个进程都有一个准确的时钟。
> **异步系统模型**：没有定时假设。进程以独立的速度执行；消息传输延迟没有上限；没有可用的时钟

当然，假设同步系统模型并不特别现实。现实世界的网络会遇到故障，消息延迟也没有严格的上限。现实世界中的系统最多最多是部分同步的，在许多入门书籍中遇见同步系统多只是因为它们在分析上更容易。

### 3.5 Consensus Problem

共识问题指的是几台计算机达成共识，即它们都统一某个值，更正式的说：

-   **协议**：每个正确的进程必须达成同一值。
-   **完整性**：每个正确的进程至多决定一个值，并且如果它决定某个值，那么这个值必须是由某个进程提出的。
-   **终止**：所有进程最终都会做出决定。
-   **有效性**：如果所有正确的进程提议相同的值 V，则所有正确的进程决定 V。

共识问题是许多分布式系统的核心，我们希望获得分布式系统的可靠性和性能，而不必处理分布带来的后果，解决共识问题使得可以解决几个相关且更高级的问题，如原子广播和原子提交。

FLP 不可能性结果是第一个不可能性结果，特别对于设计分布式算法的人来说非常重要。第二个是 CAP 定理，它是一个相关的结果，更适用于实践者——即那些需要在不同系统设计之间做出选择的人，但他们并不直接参与算法的设计。

### 3.6 FLP 不可能结果

FLP 不可能结果是说，在异步系统中，即使消息永远不会丢失，最多只有一个进程可能失败，并且它只能通过崩溃（停止执行）来失败，也不存在解决共识问题的（确定性）算法”。

FLP 不可能性结果表明，在异步系统中，如果即使一个节点可能失败，则无法通过 **确定性一致性算法** 保证达成一致。这一结果为分布式系统设计提供了理论上的限制，促使了实际中使用 **部分同步** 或 **概率性技术** 来解决一致性问题的算法的出现，如 **Paxos**、**Raft** 等，这些算法在容错设置下仍能有效地保证共识的达成。

### 3.7 CAP 定理

CAP 定理描述了分布式系统中在三个关键属性（**一致性**、**可用性**、**分区容忍性**）之间的权衡关系，指出在一个分布式系统中，最多只能同时满足其中的两个属性，而不能同时满足三个属性。

> **一致性**：要求在分布式系统中，所有节点在同一时间看到的数据是相同的。换句话说，所有操作都是同步的，即所有的数据更改在系统的所有节点之间即时同步，任何节点在读取数据时都能获取到最新的更新。
> **可用性**：要求系统始终对用户提供响应，尽管该响应可能并不是最新的值。也就是说，每次客户端请求数据时，系统会保证返回一个有效的响应，无论该请求是否可以访问到最新的数据。
> **分区容忍性**：系统能够容忍网络分区（即系统的不同节点之间的通信失败）并依然保持正常运行。当系统的某些节点由于网络问题而无法相互通信时，系统仍然能够继续提供服务并进行数据存储和更新。

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250212141309.png)

-   CA：包括完全严格的多数协议，如 2PC 两阶段提交协议。
-   CP：包括多数仲裁协议（如 Paxos），在这些协议中，少数分区不可用。
-   AP：包括使用冲突解决机制的协议，如 Dynamo。

CA 和 CP 系统设计都提供了相同的一致性模型：强一致性。唯一的区别在于，CA 系统无法容忍任何节点故障；CP 系统可以在非拜占庭故障模型下给定  `2f+1`  节点，最多容忍  `f`  个故障。

1. 早期分布式关系数据库系统中使用的许多系统设计没有考虑到分区容错性（例如，它们是 CA 设计）。
2. 网络分区期间强一致性和高可用性之间存在矛盾。
3. 强一致性与正常操作下的性能之间存在矛盾。
4. 如果我们不想在网络分区期间放弃可用性，那么我们需要探索除了强一致性之外的一致性模型是否适用于我们的需求。

网络分区是不可避免的，这就导致了一致性和可用性必须要二选一，但是如果不是一定要限制强一致性，那么也并不是非得二选一的，可以保证最终一致性。

> **ACID 一致性 != CAP 一致性**
> ACID 是数据库事务管理中的一个标准，指的是 **原子性**（Atomicity）、**一致性**（Consistency）、**隔离性**（Isolation）和**持久性**（Durability）。其中的 **一致性**（Consistency）指的是，在事务执行前后，数据库从一个合法的状态转变为另一个合法的状态。在每次操作之后，数据保持一致，符合数据库的约束和规则。
> **ACID 一致性** 是在关系型数据库和事务处理中定义的一种严格的一致性约束，它确保所有操作在数据库中的执行是严格顺序的，不允许出现部分操作完成或出现中间不一致的状态。
> **CAP 一致性** 通常指的是在分布式系统中的一致性，即 **每次读操作都返回最新的数据**，并且保证所有节点的数据一致。然而，由于 **CAP 定理** 的限制，分布式系统必须在一致性、可用性和分区容忍性之间做出选择。在面临网络分区时，系统可能会选择牺牲一致性来保证可用性，或者牺牲可用性来保证一致性。

**一致性模型**：程序员与系统之间的一种合同，其中系统保证如果程序员遵循某些特定规则，对数据存储的操作结果将是可预测的。

### 3.8 强一致性 Vs 其他一致性

一致性模型可以分为两类，强一致性和弱一致性。

1. 强一致性模型（能够维护单一副本）
    - 线性一致性
    - 顺序一致性
2. 弱一致性模型
    - 以客户为中心的一致性模型
    - 因果一致性：可用的最强模型
    - 最终一致性模型

### 3.9 强一致性模型

-   **线性一致性** 是一种非常严格的强一致性模型，它要求系统中的所有操作都遵循某个全局顺序，并且所有操作必须在某个时刻 **全局可见**。即，任何操作在系统中的执行顺序和现实世界的执行顺序必须是 **一致的**，保证了系统在逻辑上的“即时一致性”。
-   **顺序一致性** 是一种较为宽松的强一致性模型，要求系统中的所有操作必须按某种顺序进行，但不一定要严格地遵循实际发生的时间顺序。即，所有操作可以按任意顺序执行，只要它们的执行顺序对所有节点而言是一致的。
    线性一致性要求操作生效的顺序与实际操作的实时顺序相等。顺序一致性允许重新排序操作，只要每个节点观察到的顺序保持一致即可。

### 3.10 以客户为中心的一致性模型

**以客户为中心的一致性模型**关注的是客户端与服务器之间的交互一致性。这种一致性模型的重点是客户端在与系统交互时感受到的数据一致性。具体来说，它强调 **客户端看到的一致性**，而不是整个系统的全局一致性。

### 3.11 最终一致性模型

**最终一致性模型**是一种较为宽松的 **一致性模型**，其核心思想是：在没有新的写入操作的情况下，所有副本最终会达到一致状态，但这个过程可能需要一定的时间。因此，系统在某些时刻可能会表现出不一致的状态，但最终会通过后台同步操作，使所有副本的数据达到一致。

最终一致性只是一个非常宽泛的定义，表示在停止修改数据后，所有副本会在某个未定义的时间后达成一致，但在此期间副本间的数据可能是不一致的。仅仅说系统是最终一致的并没有提供足够的信息，因此需要补充以下两个方面的详细说明：

1. **“最终”的时长**：即系统达到一致的时间需要有一个明确的下界，或者至少有一个典型的时间范围。
2. **副本如何达成一致**：例如，如何选择最终的值，可能使用的方法包括“最后写入者胜”（last-writer-wins），或者其他类似的机制。正确的决策方法对于确保一致性至关重要，错误的方法可能导致数据丢失（例如，使用时间戳时如果时钟不准）。

## 4 Time and order

分布式编程是使用多台计算机解决你可以用单台计算机解决的同一个问题的艺术。这实际上正是我们对顺序关注的核心。任何只能一次执行一个操作的系统都会创建一个完全的操作顺序，每个操作都将有一个明确定义的前驱和后继。

这也是为什么 Order 受到了如此多的关注，这是因为，定义正确性的最简单方法是说“它像在单机上那样工作”，这意味着  a) 我们执行相同的操作和 b) 我们以相同的顺序执行它们——即使有多个机器。

实际上，分布式程序运行在多个节点上；具有多个 CPU 和多个操作流。你仍然可以分配一个总顺序，但这需要精确的时钟或某种形式的通信。可以通过使用完全准确的时钟为每个操作打上时间戳来决定顺序，也可以通过一个通信系统分配顺序号，就像总顺序一样。

### 4.1 全序和偏序

分布式系统中的自然状态是偏序（parital order）。网络和独立节点都不会对相对顺序做出任何保证；但在每个节点上，你可以观察到局部顺序。

1. **自反性**（Reflexivity）：在**偏序**中，对于每个元素   a ，都有   $a \leq a$ （每个元素与自身有顺序关系）。
2. **反对称性**（Antisymmetry）：与全序类似，如果   $a \leq b$  且   $b \leq a$ ，那么   a = b 。
3. **传递性**（Transitivity）：如果   $a \leq b$  且   $b \leq c$ ，那么   $a \leq c$ 。

全序（total order）是一种二元关系，为某个集合中的每个元素定义了一个顺序。

4. **全性**（Totality）：在**全序**中，对于集合中的任意两个不同元素   a  和   b ，它们一定是可比的。这意味着对于所有的   a  和   b ，要么   $a \leq b$ ，要么   $b \leq a$ （或者当   a = b  时，两者相等）。
5. **反对称性**（Antisymmetry）：如果   $a \leq b$  且   $b \leq a$ ，那么   a = b 。这个性质避免了像   $a \leq b$  且   $b \leq a$  时，  a  和   b  不相等的情况。
6. **传递性**（Transitivity）：如果   $a \leq b$  且   $b \leq c$ ，那么   $a \leq c$ 。

Git 分支是偏序的一个例子，我们从一个主分支创建多个分支，这多个分支在合并之前是没有特定顺序的，但是分支和主分支之间是有顺序的。

### 4.2 什么是时间

时间是顺序的来源——它使我们能够定义操作的顺序。

时间戳实际上是表示从宇宙开始到当前时刻世界状态的一种简写值——如果某个事件在一个特定的时间戳发生，那么它可能受到了之前发生的所有事情的影响。这个想法可以被推广为一个因果时钟，它明确地追踪原因（依赖关系），而不仅仅是假设在时间戳之前发生的所有事情都是相关的。

**Order**。当我们说时间是顺序的来源时，是说：

-   我们可以为无序事件附加时间戳以对其进行排序；
-   我们可以使用时间戳来强制操作或消息传递的特定顺序；
-   我们可以使用时间戳的值来确定某件事是否在另一件事之前按时间顺序发生。

**Interpretation**。时间作为一个普遍可比较的值。时间戳的绝对值可以被解释为一个日期，这对人们来说很有用。

**Duration**。持续时间，以时间衡量的持续时间与现实世界有一定的关联。

分布式系统的组件本质上不会以可预测的方式行为。它们不保证任何特定的顺序、前进速率或没有延迟。强加（或假设）顺序是减少可能执行和可能发生的事件空间的一种方式。当事情可以以任何顺序发生时，人类很难对其进行推理。

### 4.3 Does time Progress at the Same Rate Everywhere?

我们每个人都有基于个人经验的时间直观概念。不幸的是，这种时间的直观概念使得更容易想象全序而不是偏序。更容易想象事物依次发生，而不是并发发生。更容易推理消息的单一顺序，而不是推理以不同顺序和不同延迟到达的消息。

然而，在实现分布式系统时，我们希望避免对时间和顺序做出强烈假设，因为假设越强，系统对“时间传感器”或内置时钟的问题就越脆弱。此外，强加顺序会带来成本。

#### 4.3.1 具有“全局时钟”假设的时间

全局时钟假设是指存在一个完美准确的全局时钟，每个人都能访问该时钟。然而，这是对世界的理想化看法：实际上，时钟同步只能达到一定程度的准确性。这受到商品计算机中时钟准确性不足的限制，从根本上说，还受到时空本质的限制。

在故障情况下，比如用户不小心更改了机器上的本地时间，或者一台过时的机器加入集群，或者同步的时钟以略微不同的速率漂移等等，都会导致这种假设下的系统的异常。但是实际上也有一些系统确实采用了这种假设。

#### 4.3.2 具有“本地时钟”假设的时间

更合理的假设是每台机器都有自己的时钟，但没有全局时钟。这意味着你不能使用本地时钟来确定远程时间戳是在本地时间戳之前还是之后；换句话说，你不能有意义地比较来自两台不同机器的时间戳。

本地时钟假设更贴近现实世界。它分配了一个偏序：每个系统中的事件是有序的，但仅使用时钟无法跨系统对事件进行排序。

#### 4.3.3 具有“无时钟”假设的时间

最后，有一个逻辑时间的概念。在这里，我们根本不使用时钟，而是以其他方式追踪因果关系。

我们用计数器和通信来确定某件事是发生在之前、之后还是与另一件事同时发生。这样，我们可以确定不同机器之间事件的顺序，但不能说明间隔，也不能使用超时（因为我们假设没有“时间传感器”）。这是一个偏序：事件可以在单个系统中使用计数器且无需通信进行排序，但在系统之间排序事件则需要消息交换。

### 4.4 时间在分布式系统中是如何使用的

时间可以定义系统中的顺序（无需通信），时间可以为算法定义边界条件。

一个全局时钟将允许在两台不同机器上的操作按顺序进行，而无需这两台机器直接通信。没有全局时钟，我们需要通信来确定顺序。

时间也可以用于定义算法的边界条件——特别是区分“高延迟”和“服务器或网络链接已断开”。这是一个非常重要的用例；在大多数实际系统中，超时用于确定远程机器是否已失败，或者它是否只是经历高网络延迟。做出此判断的算法称为故障检测器。

### 4.5 Vector Clocks

在分布式系统中，由于无法实现精确的物理时钟同步，我们需要一种逻辑时钟机制来对事件进行排序，尤其是捕获因果关系。

Lamport 时钟和向量时钟是物理时钟的替代品，它们依赖计数器和通信来确定分布式系统中事件的顺序。Lamport 时钟中每个进程使用以下规则维护一个计数器：

1. 每当一个进程执行工作时，增加计数器；
2. 每当一个进程发送消息时，包含计数器；
3. 当收到消息时，将计数器设置为  `max(local_counter, received_counter) + 1`。

Lamport 时钟允许在系统之间比较计数器，但有一个前提：Lamport 时钟定义了一个偏序。如果  `timestamp(a) < timestamp(b)`，表示 a 可能发生在 b 之前或者 a 可能与 b 不可比较。

但是，这种时钟无法完全捕获因果关系；如果两个事件的计数器值相同，也无法确定它们的顺序；对于不通信的独立子系统，Lamport 时钟无法比较它们的事件顺序。

**向量时钟**是 Lamport 时钟的扩展，它维护一个数组  `[ t1, t2, ... ]` ，包含 N 个逻辑时钟——每个节点一个。每个节点在每次内部事件时都会将其向量中的自有逻辑时钟加一，而不是增加一个公共计数器。

4. 每当一个进程执行操作时，增加向量中节点的逻辑时钟值；
5. 每当一个进程发送消息时，包含完整的逻辑时钟向量；
6. 当收到消息时将向量中的每个元素更新为  `max(local, received)`；将向量中表示当前节点的逻辑时钟值加一。

向量时钟的特点是能够精确捕获**因果关系**：

-   如果事件 A 的向量在所有维度上都小于或等于事件 B 的向量，并且至少有一个维度严格小于，则 A 发生在 B 之前。
-   如果两个事件的向量无法比较，则它们是并发的。

下图显示了一个向量时钟：

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250212163200.png)

每个节点（A，B，C）都跟踪向量时钟。事件发生时，它们会被标记上当前的向量时钟值。检查一个向量时钟如  `{ A: 2, B: 4, C: 1 }` ，可以让我们准确地识别出（可能）影响该事件的消息。

向量时钟的问题主要是它们需要每个节点一个条目，这意味着对于大型系统来说，它们可能会变得非常大。

### 4.6 Failure Detectors (time for cutoff)

在分布式系统中，如何判断远程节点是否故障，尤其是在没有全局时钟和精确时间同步的情况下。**故障检测器**通过抽象化时间假设，帮助系统推断节点是否失效。

分布式系统中，节点可能因故障或网络分区而不可用。需要区分节点是故障还是仅仅经历高延迟。

> 使用**心跳消息**和**超时机制**来检测节点是否失效。如果在一定时间内未收到节点的响应，则怀疑该节点已故障。

故障检测器应该具有两个核心属性：

1. **完备性**：每个故障节点最终都会被所有（强完备性，弱完备性是至少一个）正常节点怀疑。
2. **准确性**：所有（强准确性，弱准确性是至少一个）正常节点永远不会被怀疑。

> **最终弱故障检测器（⋄W）**：即使是非常弱的故障检测器，也可以用于解决共识问题。

### 4.7 时间、顺序和性能

虽然时间和顺序经常一起讨论，但时间本身并不是一个很有用的属性。算法更关心的不是时间，而是更抽象的属性：

1. 事件的因果顺序；
2. 故障检测（例如，消息传递上限的近似值）；
3. 一致的快照（例如，检查系统在某个时间点状态的能力）。

## 5 Replication

复制问题是分布式系统中一个非常重要的问题，因为它直接影响到系统的性能、可用性和数据一致性。此外，复制不仅仅是数据拷贝，它还涉及到许多子问题，如领导者选举（决定哪个节点负责协调复制）、故障检测（检测节点是否失效）、共识（多个节点就某个值达成一致）和原子广播（确保消息以原子方式广播到所有节点）。

复制问题是一个**组通信**问题，即如何在多个节点之间安排和协调通信，以达到所需的性能和可用性。主要挑战包括在网络分区（网络断开导致节点无法通信）和节点故障的情况下，如何确保系统的容错性（系统在部分失效时仍能工作）、持久性（数据不会丢失）和非分歧性（所有节点的数据保持一致）。

### 5.1 同步复制（Synchronous）

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250213150655.png)

这里，可以分为三个不同的阶段。首先，客户端发送请求；然后是复制的同步部分（客户端被阻塞，等待系统同步结束）；最后，向客户端发送一个响应，告知结果。

这是一种 N 写 N 的复制方式，在返回响应之前，必须由系统中的每个服务器看到并确认。性能很慢，需要等待最慢的那台服务器并且对网络延迟非常敏感。系统也**无法忍受任何服务器的丢失**，如果有丢失，则系统无法写入数据。

### 5.2 异步复制（Asynchronous）

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250213151446.png)

异步复制中，主节点（/领导者 / 协调者）**立即**向客户端发送响应。它最多可能仅本地存储更新，但不会同步执行任何重要工作，客户端也不必等待服务器之间发生更多轮的通信。

这是一种 N 写 1 的复制方法，立即返回响应，更新传播稍后发生。系统运行速度快，也能容忍网络延迟，但是这种安排只能提供弱或者概率性的持久化保证。如果在数据复制之前，唯一包含数据的服务器丢失了，那么数据将永久丢失。并且，被动复制无法确保系统中的所有节点始终包含相同的状态。

### 5.3 主要复制算法概述

有许多不同的方式来对复制技术进行分类，除了同步和异步之外，还有一种分类是：

-   防止数据分歧的复制方法（单副本系统）
-   可能导致数据分歧的复制方法（多主系统）

单副本系统确保系统中所有副本的数据始终保持一致，不会出现数据分歧（即不同副本之间的数据不一致）。通常采用**单主复制(Single-Master Replication)** 的方式，即只有一个主节点（Master）负责处理写操作，其他副本（Slaves）只能读取数据或从主节点同步数据。

系统确保副本始终达成一致，这被称为**共识问题**。共识问题要求多个进程在某个值上达成一致，即使在某些进程可能失败或网络不可靠的情况下。为了实现共识，必须满足以下四个核心条件：

-   **Agreement（一致性）**：所有正确的进程（即没有发生故障的进程）必须就同一个值达成一致。
-   **Integrity（完整性）**：每个正确的进程最多只能决定一个值，并且如果它决定了某个值，那么这个值必须是由某个进程提出的。
-   **Termination（终止性）**：所有进程最终都必须达成一个决定。
-   **Validity（有效性）**：如果所有正确的进程都提出了相同的值  V，那么所有正确的进程最终都必须决定  V。

互斥、领导者选举、多播和原子广播都是更一般的共识问题的实例。

下面是**维护单副本一致性（Single-Copy Consistency）** 的几种复制算法及其通信开销：

4. **1n 消息（异步主备复制，Asynchronous Primary/Backup）**。每条写操作需要发送 1n 条消息，其中 n 是副本的数量。主节点（Primary）接收写请求并更新本地数据；主节点异步地将更新发送给备份节点（Backups），而不等待备份节点的确认。
5. **2n 消息（同步主备复制，Synchronous Primary/Backup）**。每条写操作需要发送 2n 条消息。主节点接收写请求并更新本地数据；主节点将更新同步发送给所有备份节点，并等待它们的确认；只有在收到所有备份节点的确认后，主节点才向客户端返回成功。
6. **4n 消息（两阶段提交，2-Phase Commit，2PC / Multi-Paxos）**。每条写操作需要发送 4n 条消息。
    > **两阶段提交（2PC）**:
    > **准备阶段**：协调者（Coordinator）向所有参与者（Participants）发送准备请求，并等待它们的响应。
    > **提交阶段**：如果所有参与者都同意，协调者发送提交请求；否则，发送回滚请求。
    > **Multi-Paxos**：
    > Paxos 是一种分布式共识算法，Multi-Paxos 是其优化版本，通过多轮消息交换，确保所有节点就某个值达成一致。
7. **6n 消息（三阶段提交，3-Phase Commit，3PC / Paxos with repeated leader election）**。三阶段提交的准备阶段协调者向所有参与者发送准备请求；预提交阶段协调者确认所有参与者已经准备好，并发送预提交请求；提交阶段协调者发送最终提交请求。

![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250213164812.png)

这张图片展示了四种分布式系统协议（M/S、Gossip、2PC、Quorum）在几个维度上的对比：一致性、事务、延迟、吞吐量、数据丢失和故障转移。（只读故障转移：主节点故障后，只能进行读取操作；读写故障转移：节点故障后，仍然可以执行读写操作）

### 5.4 主/备份复制

主/备份复制（也称为主副本复制或日志传送）或许是使用最广泛的复制方法，也是最基本的算法。所有的更新都在主服务器上进行，并将操作日志通过网络发送到备份副本。分为同步和异步两种。

这种模式非常常见，例如，默认情况下 MySQL 复制使用的就是异步的变体。异步复制算法只提供较弱的持久性保证，在 MySQL 复制中，表现为复制延迟：异步备份总是至少落后于主库一个操作。如果主库发生故障，那些尚未发送到备份的更新将会丢失。

就算是同步变体，也会有问题，如主节点将请求转发到备份节点，备份节点以及确认写入了，这个时候主节点向客户端发送 ACK 失败。客户端现在认为提交失败，但备份已提交；如果备份被提升为主节点，将会出现错误。

主备方案容易遇到**脑裂问题**，即由于临时网络问题触发备份接管，导致主节点和备份节点同时处于活动状态。

### 5.5 两阶段提交（2PC）

两阶段提交（2PC）是许多经典关系型数据库中使用的协议。例如，MySQL Cluster（不要与常规的 MySQL 混淆）使用 2PC 提供同步复制。

第一阶段（投票阶段）：协调者（Coordinator）向所有参与者（Participants）发送“准备提交”请求。每个参与者处理请求，并将事务日志写入预写日志（Write-Ahead Log, WAL），然后向协调者返回“同意”或“中止”的投票结果。

第二阶段（决策阶段）：如果所有参与者都同意提交，协调者发送“提交”请求，参与者将事务从临时状态转为永久状态。如果有任何一个参与者返回“中止”，协调者发送“回滚”请求，参与者撤销事务。

-   **强一致性**：2PC 确保所有节点要么全部提交，要么全部回滚，避免了数据不一致。
-   **故障恢复**：通过预写日志和两阶段机制，2PC 可以在节点故障后恢复事务状态。
-   **阻塞问题**：如果协调者或某个参与者在第二阶段失败，系统可能会阻塞直到故障节点恢复。
-   **无分区容忍性**：2PC 是 CA 系统（一致性和可用性），不支持网络分区。

### 5.6 分区容忍共识算法

在分区容忍的共识算法中，最著名的算法是 Paxos 算法也包括新出现（本文 2013 年）的 Raft 算法。

网络分区是指由于网络链路故障，导致分布式系统中的部分节点无法与其他节点通信。这些节点可能仍然在运行，并且能够与它们所在分区内的客户端通信。如果系统没有机制来打破对称性，网络分区可能导致系统分裂成多个独立运行的分区，这些分区可能会产生数据分歧（即数据不一致）。对于强制实施单副本一致性的系统，网络分区期间必须确保只有一个分区保持活跃，以避免数据分歧。又根据 CAP 定理，分布式系统在网络分区发生时，只能在一致性和可用性之间二选一。因此，为了维护一致性，必须牺牲可用性，即只允许一个分区保持活跃，其他分区必须停止服务。

#### 5.6.1 多数决策

分区容忍共识算法依赖于多数投票，要求大多数节点（而不是所有节点，如 2PC）就更新达成一致，允许少数节点因网络分区而宕机、变慢或无法访问。只要过半节点正常运行且可访问，系统就能继续运作。

多数派机制之所以有用，还在于它们能够容忍分歧：若出现扰动或故障，节点可能会有不同的投票结果。然而，由于**最终只能形成一个多数决策**，暂时的分歧最多只能阻止协议继续推进（牺牲活性），但不会违反单副本一致性准则（安全性属性）。

#### 5.6.2 角色

在分布式系统中，节点的角色设计有两种主要方式：

- **对称角色（所有节点职责相同）**：
    所有节点具有相同的职责，可以处理相同的任务。
    例如，在无主复制（Leaderless Replication）系统中，所有节点都可以直接处理读写请求。
 - **非对称角色（节点职责不同）**：
    节点被分配不同的角色，例如领导者（Leader）和跟随者（Follower）。
    例如，在主从复制（Primary/Backup Replication）或共识算法（如 Paxos、Raft）中，领导者负责协调写操作，跟随者负责同步数据。

共识算法（如 Paxos、Raft）通常采用非对称角色设计，一方面，通过固定一个领导者（Leader），所有写操作都通过领导者协调，避免了多个节点之间的冲突和协调开销。另一方面，非领导者节点只需将请求转发给领导者，简化了系统的复杂性。

#### 5.6.3 Epochs

Epochs（在 Raft 中称为“任期”）是分布式一致性算法（如 Paxos 和 Raft）中的一个重要概念，用于描述系统正常运行的时间周期。每个 epoch 代表一个特定的时间段，期间只有一个节点被指定为领导者，负责协调系统的操作。以下是 epochs 的关键点概括：

1. **领导者选举**：在每个 epoch 开始时，系统会通过选举选出一个领导者。如果选举成功，该领导者将负责协调系统操作直到该 epoch 结束。如果选举失败，epoch 会立即结束，系统会尝试重新选举。
2. **逻辑时钟**：Epochs 充当逻辑时钟，帮助系统识别节点的状态。每个 epoch 都有一个唯一的编号，节点通过比较 epoch 编号来判断信息是否过时。如果一个节点的 epoch 编号小于当前 epoch，说明该节点可能经历了分区或故障，其发出的命令将被忽略。
3. **故障处理**：当节点因网络分区或故障而重新加入系统时，它们的 epoch 编号可能落后于当前 epoch。系统通过 epoch 编号识别这些过时节点，并确保它们不会干扰当前的操作。

总结来说，epochs 是分布式系统中用于管理领导者选举、协调操作和处理故障的逻辑时间单位，确保系统的一致性和可靠性。

