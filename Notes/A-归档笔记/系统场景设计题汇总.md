## 1 大数据存储查询设计

**问题**：每天 TB 级玩家行为日志，需数据库存储并支持按时间段、玩家 ID 等条件快速查询。

**分析**：应对海量数据、高效索引和查询性能挑战。需分布式系统。

> [!NOTE] 数据库选型
> **MySQL**：基于 B+ 树，擅长读多写少场景的关系型数据库，不适合海量数据存储。
> **LevelDB**：基于 LSM 树的 KV 存储，写入性能高，但不支持 SQL 且查询能力有限。
> **InfluxDB**：时间序列数据库，写入快，时序查询高效，适合存储时间相关数据。
> **Elasticsearch**：基于倒排索引 + LSM 树，擅长全文检索和复杂分析。

**总体思路**：分层设计：数据采集、数据存储、索引与查询、扩展运维。

### 1.1 数据采集层

日志是实时产生的，需要低延迟、高吞吐量的写入通道，同时避免单点瓶颈。

- 使用分布式消息队列（如 Kafka）缓冲实时日志。
- 日志按（时间 + 玩家 ID）哈希分散到主题/分区。
- 日志包含：玩家 ID、时间戳、行为类型、附加数据（JSON）。

**优化**：批量写入、消息队列持久化。

### 1.2 数据存储层

TB 级数据需要分布式存储，结合时间序列和玩家 ID 的查询特点，选择合适的存储引擎。

- 选用分布式时间序列数据库，适合时间段查询。
- 数据按时间分区存储在分布式节点。
- 字段：时间戳（主键）、玩家 ID、行为数据。

**优化**：冷热分离（SSD、HDFS）、数据分片、预聚合、内存缓冲、异步批量写入。

### 1.3 索引与查询层

快速查询需要高效索引，时间段和玩家 ID 是主要条件，要针对这两种模式优化。

- 时间序列数据库自带时间索引，高效查询时间范围。
- 构建二级索引：分布式 KV 存储（Redis）维护玩家 ID 到时间戳的映射。

**优化**：缓存热点数据、异步更新索引。

### 1.4 拓展运维层

系统需支持数据增长和高可用，同时便于维护。

- **水平扩展**：存储和索引节点动态扩容，自动分配分片。
- **高可用**：数据多副本存储，分布在不同节点，容忍单点故障。
- **监控优化**：监控延迟、吞吐量，动态调整分片或资源。

**核心**：分而治之，多层次优化。

- **分而治之**：采集、存储、查询分层。
- **优化重点**：复合分区负载均衡，批量写入减压，索引加速查询。
- **结果**：高效支持 TB 级存储与亿级日活查询。

## 2 实时在线人数统计

**问题**：实时统计日活约一亿的游戏全服每分钟在线人数。

**分析**：高并发实时数据聚合与低延迟查询的挑战。

**总体思路**：利用流式处理引擎实时聚合玩家心跳数据，并使用分布式计数器进行统计。

### 2.1 数据采集层

- 游戏服务器发送玩家心跳数据（包含玩家 ID 和时间戳）至分布式消息队列（如 Kafka）。
- 心跳数据按玩家 ID 或服务器 ID 分区，提高并行处理能力。

**优化方向**：轻量级心跳协议，高吞吐写入保障。

### 2.2 数据聚合层

- 使用流式处理系统（如 Flink）实时消费 Kafka 中的心跳数据。
- **时间窗口划分**：Flink 按分钟划分时间窗口处理心跳数据流。
- **去重计数**：每个 Flink 任务可以使用 Redis 的 HyperLogLog 对窗口内的玩家 ID 进行近似去重计数，或使用精确去重方案（如 Set）。
- **分片聚合**：每个 Flink 任务负责一部分分区的数据聚合。
- **结果汇总**：将各 Flink 任务的统计结果发送至一个或多个全局计数器（如 Redis 集群）。

**优化方向**：选择合适的去重方案（精度 vs. 资源），状态管理，容错机制。

### 2.3 在线人数存储与查询层

- 使用高并发低延迟的分布式 KV 存储（如 Redis 集群）作为全局计数器，存储每分钟的在线人数。
- 后端服务直接读取 Redis 中的数据，对外提供实时在线人数查询接口。

**优化方向**：内存优化，读写分离，多级缓存。

**核心**：流式计算，分片聚合，分布式计数。

- **流式计算**：实时处理心跳数据流。
- **分片聚合**：分散计算压力，提高处理效率。
- **分布式计数**：提供高并发的计数服务。
- **结果**：实时、准确地统计高日活游戏的全服每分钟在线人数。

## 3 秒杀场景如何保证消息唯一性

如何确保消息唯一的投递？即考虑网络带来的重传（用分布式 ID），也考虑用户的多次误点击（用消息哈希去重）。最后结合幂等检查来处理。

- **使用 MurmurHash 哈希消息内容**，将 `userID + hash` 作为 Redis 的 key，并设置过期时间。
- **检查 Redis 中的重复 key**，如果存在，则判定为重复消息，拒绝投递。
- **生成分布式唯一 ID**，使用雪花算法为消息生成唯一 ID，确保消息在分布式系统中的唯一性。
- **投递到支持消息去重的消息队列**，如 Kafka 或 RabbitMQ 的幂等性生产者，配置消息的唯一 ID，防止重复投递。
- **增加幂等性检查**，在处理消息前，检查 MySQL 中是否已记录该消息 ID，确保消息只被处理一次。
- **结合 Redis 和消息队列**，通过双重机制确保消息的唯一投递和幂等性处理。

## 4 高并发秒杀场景（演唱会抢票）
![image.png](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/20250329131702.png)
左边部分主要偏向于编程应用，右边部分偏向于组件应用。

**问题**：设计一个高并发的演唱会抢票系统，应对瞬时巨大的流量冲击。

**分析**：应对瞬时高并发、库存竞争、系统稳定性等挑战。需分布式架构和多重保护机制。

**总体思路**：分层设计，从流量接入到最终数据存储，每一层都进行优化和保护。

### 4.1 客户端请求层

用户通过浏览器或 App 发起抢票请求（包含用户 ID、场次 ID、座位信息等）。
- **前端限流**：按钮点击防抖、一定时间内禁止重复提交。
- **页面静态化**：将活动页面、图片等静态资源部署到 CDN，加速用户访问。

### 4.2 接入层/网关

所有用户请求首先到达接入层/网关。

- **负载均衡**：将海量请求分发到后端的多个抢票服务实例（采用一致性哈希等策略）。
- **认证鉴权**：验证用户身份。
- **第一层限流**：使用**令牌桶**等算法限制单位时间内的总请求量。

### 4.3 抢票服务层

接收网关转发的抢票请求，处理抢票逻辑、库存管理。

- **第二层限流**：针对单个用户或特定场次再次进行**令牌桶**等限流。
- **缓存检查（Redis）**：查询 Redis 中该场次是否还有剩余库存。
- **预扣库存（Redis 原子操作）**：使用 Redis 的原子操作（如 Lua 脚本）尝试扣减库存。
- **请求入队（消息队列）**：如果 Redis 预扣库存成功，将抢票请求（用户 ID、场次 ID、座位信息等）封装成消息发送到**消息队列**（如 Kafka）进行异步处理。
- **预扣失败**：如果 Redis 库存不足或扣减失败，直接返回 " 已售罄 " 等提示。

**优化方向**：熔断降级

### 4.4 订单服务层

消费消息队列中的抢票成功消息，创建订单、更新数据库库存。

- **库存最终校验（数据库）**：从数据库中再次校验实际库存，防止超卖（极端情况）。
- **生成订单**：创建订单记录并写入数据库。
- **更新数据库库存**：减少数据库中的实际库存。
- **发送支付通知（可选）**：通知用户进行支付。

### 4.5 拓展运维层

- **服务注册与发现（ETCD/Consul）**：动态管理服务实例。
- **健康检查**：保障服务可用性。
- **弹性伸缩（Kubernetes）**：根据流量动态调整服务实例数量。
- **监控告警（Prometheus + Grafana）**：实时监控系统状态，及时发现和处理问题。
- **日志分析（ELK）**：集中管理和分析日志，方便故障排查。

**核心**：分层限流，高速缓存，异步处理，最终一致性。

- **分层限流**：从网关到服务内部进行多层限流，保障系统整体稳定性。
- **高速缓存**：利用 Redis 缓存高频访问的数据，减轻数据库压力。
- **异步处理**：通过消息队列将非核心流程异步化，提升系统吞吐量。
- **最终一致性**：通过消息队列和数据库操作保证最终的库存和订单数据一致。
