我使用 Go 语言实现一个分布式的 IM 系统，具体来说，系统分为四个层：
API 层负责接收用户的注册、登录请求，和单聊/群聊消息的发送。主要使用 GIN 开发。
Logic 层负责处理后端逻辑，如注册登录的验证、数据库和 redis 的操作等。在收到消息后， 为消息生成一个分布式 ID （snowflakeId）用于唯一标识，将消息传入消息队列。
Task 层从消息队列中消费消息，分为两个消息组，一个消息组消费消息，将其写入到通道，用于发送给 Connect 端。另一个消费组从消息队列中取出消息，对消息进行持久化，保存到数据库中。
Connect 层负责为用户提供 websocket 长连接。用户连接或断开连接后，Connect 会给 Logic 发送消息，Logic 更新房间成员信息并将广播消息打包发送到消息队列。此外，Connect 接收 Task 中传输来的消息，并通过 websocket 传输给用户。

API 层和 Logic 层之间，Task 和 Connect 之间通过 gRPC 通信，Logic 层和 Connect 层都启动相应的 RPC 服务器。并使用 etcd 的（租约、自动续约、Watch 机制）进行服务注册和发现。我设计的服务注册和发现有两种模式：基于 gRPC resolver 的自动负载均衡模式和基于 InstanceID 的定向连接模式。logic 没有状态，可以使用前一种服务发现。而 Connect 上有状态，发送给某个用户的消息我们只能发送给特定的 Connect 实例，而不是全部实例。因此，使用第二种模式。使用懒连接机制，维护连接管理器按需创建并复用连接资源，并通过 Watch 机制实现服务实例的动态伸缩。

消息队列使用的是 Redis Stream 实现的，并支持消费组，方便 Task 层进行业务扩展。当前我们的系统有两个消费者组，一个推送消息，一个持久化消息到数据库中。利用 Redis Stream 的持久化特性，确保消息不丢失；实现消息确认机制，只有成功处理的消息才会被确认删除。采用 Redis 消费者组设计，支持多消费者协同工作，避免重复消费问题；利用 XReadGroup、XAck 等原生命令实现精确的消息分配和确认。

主要涉及到的库包括 gin、websocket 用于用户连接、grpc、etcd 内部通信和服务发现、redis 用于缓存消息（房间信息、userid 和 instanceid 对应关系）和消息队列、jwt（生成 token）、gorm 和 mysql 作为数据库操作。

最后，使用 docker-compose 部署系统，包括三台 etcd 服务器、mysql、redis 服务器、三台 api 服务器，三台 Logic 服务器、三台 Task 服务器和三台 Connect 服务器。用户通过 nginx 负载均衡连接到 api 和 Connect 层。

一次消息发送的流程如下：

```go
消息发送必须在登录状态下,用户A向用户B发送一条消息。需要经历如下历程：
1,用户A调用api层接口登录系统,登录成功后与connect层认证保持长链接,
并rpc call logic层记录用户A在connect层登录的serverId,默认加入房间号1

2,用户B调用api层接口登录系统,登录成功后与connect层认证保持长链接,
并rpc call logic层记录用户B在connect层登录的serverId,默认加入房间号1

3,用户A调用api层接口发送消息,api层rpc call logic层发送消息方法,
logic层将该消息push到队列中等待task层消费

4,task层订阅logic层发送到队列中的消息后,
根据消息内容(userId,roomId,serverId)可以定位用户B目前在connect层那一个serverId上保持长链接,
进一步rpc call connect层方法

5,connect层被task层rpc call之后,会将该消息投递到相关房间内，
再进一步投递给在该房间内的用户B,完成一次完整的消息会话通讯

6,如果是私信消息,那么task层会根据userId定位connect层对应的serverId,
直接rpc call 该serverId的connect层，完成私信消息投递。
```
