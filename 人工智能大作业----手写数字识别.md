# 手写数字识别大作业

## 实验目的

1.  **掌握 MNIST 数据集的加载与预处理方法**：理解 MNIST 数据集的结构，掌握如何使用编程工具加载数据，并进行必要的预处理操作，如数据标准化、归一化等，为后续模型训练提供高质量的输入数据。
2.  **理解并实现 KNN 算法、全连接神经网络（MLP），包括使用库和自行实现**：深入理解 KNN 和 MLP 的原理，掌握使用现有库（如 Scikit-learn）实现这些算法的方法，并能够通过编程自行实现这些算法，加深对算法内部机制的理解。
3.  **比较不同算法在分类准确率、运行效率和参数敏感性方面的性能差异**：通过实验对比 KNN 和 MLP 模型在手写数字识别任务上的性能差异，包括分类准确率、训练和测试时间，以及不同参数（如 KNN 的 k 值、MLP 的隐藏层结构和学习率）对模型性能的影响。
4.  **学会通过实验数据整理和曲线绘制分析各模型的性能特点**：掌握如何整理实验数据，使用图表（如准确率曲线、训练时间曲线）可视化实验结果，并根据图表分析不同模型的性能特点和适用场景。

## 实验方法

### KNN 算法

K 近邻（KNN）算法是一种基于实例的学习方法，其核心思想是“物以类聚”。在分类任务中，KNN 算法会将待分类的样本归类到与其最近的 K 个邻居中出现次数最多的类别。该算法不需要显式的训练过程，而是直接利用训练数据集进行预测。为了确保算法的有效性，我们需要对数据进行预处理。首先，所有样本的特征都需要进行量化处理，即将非数值类型的特征转换为数值类型，例如，颜色特征可以转换为灰度值。其次，样本特征需要进行归一化处理，以消除不同特征之间量纲和取值范围差异的影响。这是因为，如果不同特征的取值范围差异较大，那么在计算距离时，取值范围较大的特征会占据主导地位，从而影响模型的准确性。常用的归一化方法包括最小-最大归一化和 Z-score 标准化。在 KNN 算法中，距离函数的选择至关重要，常用的距离函数包括欧氏距离、余弦距离、汉明距离和曼哈顿距离等。对于连续变量，通常使用欧氏距离，而对于非连续变量，可以使用汉明距离。此外，K 值的选择也会对模型的性能产生重要影响。K 值过大会导致欠拟合，而 K 值过小则容易过拟合。因此，需要通过交叉验证等方法来选择最佳的 K 值。

### MLP 多层感知机

多层感知机（MLP）是一种前馈神经网络，它由输入层、一个或多个隐藏层和输出层组成。MLP 的核心思想是通过非线性激活函数引入非线性，使得网络能够学习复杂的特征关系。在 MLP 中，每个神经元都与下一层的所有神经元相连接，这种连接方式被称为全连接。MLP 的核心组成部分包括权重、偏置和激活函数。权重表示神经元之间连接的强度，偏置是模型中的一个重要参数，用于帮助模型更好地拟合数据，而激活函数的作用是引入非线性，使神经网络能够学习复杂的特征关系。常用的激活函数包括 Sigmoid 函数、ReLU 函数和 Tanh 函数等。MLP 的训练过程是通过反向传播算法来更新权重和偏置。在训练开始时，网络中的权重会被随机初始化。对于训练数据中的每个输入，神经网络会生成相应的输出，并与期望的目标值进行比较。误差会通过反向传播传递到上一层，根据误差调整权重。这个过程会不断重复，直到误差收敛为止，从而完成模型的训练。

## 数据集准备和预处理

首先，我们从 `https://yann.lecun.com/exdb/mnist/` 下载 MNIST 数据集，该网站提供了训练集图像、训练集标签、测试集图像和测试集标签四个压缩文件。这些文件以 `.gz` 格式压缩，需要解压缩才能使用。

为了方便加载数据，我们定义了一个名为 `load_mnist_data` 的函数，该函数接收图像文件路径和标签文件路径作为输入，并返回图像数据和标签数据。该函数使用 `gzip` 库打开压缩文件，并使用 `struct.unpack` 函数读取文件头信息，包括魔数（magic number）、样本数量、图像行数和列数。然后，使用 `np.frombuffer` 函数读取图像数据和标签数据，并将其转换为 NumPy 数组。对于图像数据，我们还需要使用 `reshape` 函数将其转换为 `(样本数量, 行数 * 列数)` 的二维数组，以便后续处理。

以下是 `load_mnist_data` 函数的具体实现：

```python
def load_mnist_data(image_file, label_file):
    # 加载图像
    with gzip.open(image_file, 'rb') as f:
        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))
        images = np.frombuffer(f.read(), dtype=np.uint8)
        images = images.reshape(num, rows * cols)
    # 加载标签
    with gzip.open(label_file, 'rb') as f:
        magic, num = struct.unpack('>II', f.read(8))
        labels = np.frombuffer(f.read(), dtype=np.uint8)
    return images, labels
```

接下来，我们使用 `load_mnist_data` 函数加载训练集和测试集数据：

```python
# 加载数据
train_images, train_labels = load_mnist_data('train-images-idx3-ubyte.gz', 
                                           'train-labels-idx1-ubyte.gz')
test_images, test_labels = load_mnist_data('t10k-images-idx3-ubyte.gz', 
                                         't10k-labels-idx1-ubyte.gz')
```

在加载数据之后，我们需要对数据进行预处理，以提高模型的训练效果。在本实验中，我们使用 StandardScaler 对图像数据进行标准化处理。标准化处理的目的是将数据转换为均值为 0，标准差为 1 的分布，这样可以消除不同特征之间的量纲差异，提高模型的训练速度和准确率。

```python
# 数据预处理
scaler = StandardScaler()
X_train = scaler.fit_transform(train_images.astype('float32'))
X_test = scaler.transform(test_images.astype('float32'))
y_train = train_labels
y_test = test_labels
```

首先，我们创建一个 `StandardScaler` 对象。然后，使用 `fit_transform` 方法对训练集数据进行标准化处理，并使用 `transform` 方法对测试集数据进行标准化处理。这里需要注意的是，我们首先将图像数据的类型转换为 `float32`，然后再进行标准化处理。最后，我们将标签数据分别赋值给 `y_train` 和 `y_test` 变量。

## KNN 模型实现

### 代码说明

这段代码定义了一个名为 `knn_experiment` 的函数，该函数接收训练集数据、测试集数据、训练集标签、测试集标签和一个 K 值列表作为输入，并返回一个包含所有实验结果的列表。该函数首先初始化一个空列表 `results`，用于存储实验结果。然后，使用 `for` 循环遍历所有可能的 K 值。对于每个 K 值，函数会创建一个 `KNeighborsClassifier` 对象，并使用训练集数据对其进行训练。在训练和预测过程中，函数会记录训练时间和测试时间，并使用 `accuracy_score` 函数计算模型的准确率。最后，将所有实验结果存储在 `results` 列表中并返回。

```python
from sklearn.neighbors import KNeighborsClassifier

def knn_experiment(X_train, X_test, y_train, y_test, k_values):
    results = []
    for k in k_values:
        print(f"正在测试 k={k}...")
        knn = KNeighborsClassifier(n_neighbors=k)
        start_time = time.time()
        knn.fit(X_train, y_train)
        train_time = time.time() - start_time
        
        start_time = time.time()
        y_pred = knn.predict(X_test)
        test_time = time.time() - start_time
        
        accuracy = accuracy_score(y_test, y_pred)

        results.append({
            'k': k,
            'accuracy': accuracy,
            'train_time': train_time,
            'test_time': test_time
        })
    return results

# 测试不同的k值
k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]
knn_results = knn_experiment(X_train, X_test, y_train, y_test, k_values)
```

### 测试结果

本部分主要展示了 KNN 算法在不同 K 值下的性能表现，包括分类准确率、训练时间和测试时间。为了方便分析，我们使用了表格和图表两种方式来呈现结果。

首先，我们来看一下用于生成测试结果的代码。代码主要分为三个部分：打印 KNN 算法的详细结果、打印最佳结果和使用 Matplotlib 绘制结果图。

#### 打印 KNN 详细结果

这部分代码首先打印一个标题，然后打印表头，包括 K 值、准确率、训练时间和测试时间。接着，遍历 `knn_results` 列表，该列表包含了不同 K 值下的实验结果。对于每个结果，代码会提取 K 值、准确率、训练时间和测试时间，并按照指定格式打印出来。为了方便比较，我们使用 `sorted` 函数按照准确率降序排列结果。

```python
# 打印KNN实验结果
print("\n=== K近邻分类器测试结果 ===")
print("-" * 60)
print(f"{'k值':<6} {'准确率':<10} {'训练时间(s)':<12} {'测试时间(s)':<12}")
print("-" * 60)

# 按准确率排序
sorted_results = sorted(knn_results, key=lambda x: x['accuracy'], reverse=True)

# 打印每组结果
for result in sorted_results:
    k = str(result['k'])
    acc = f"{result['accuracy']*100:.2f}%"
    train = f"{result['train_time']:.3f}"
    test = f"{result['test_time']:.3f}"
    print(f"{k:<6} {acc:<10} {train:<12} {test:<12}")

print("-" * 60)
```

这部分代码用于打印最佳的 KNN 算法结果，包括最佳 K 值、最佳准确率、最佳训练时间和最佳测试时间。我们首先使用 sorted 函数按照准确率降序排列结果，然后取第一个结果作为最佳结果。

```python
# 打印最佳配置
best = sorted_results[0]
print(f"\n最佳配置:")
print(f"k值: {best['k']}")
print(f"准确率: {best['accuracy']*100:.2f}%")
print(f"训练耗时: {best['train_time']:.3f}秒")
print(f"预测耗时: {best['test_time']:.3f}秒")
```

#### 使用 Matplotlib 绘制结果图

这部分代码使用 Matplotlib 库绘制 K 值与准确率之间的关系图。为了在图中正确显示中文，我们首先设置中文字体。然后，我们定义一个 `plot_knn_results` 函数，该函数接收实验结果列表作为输入，并提取 K 值和准确率，然后使用 `plot` 函数绘制折线图。最后，我们设置图表的标题、X 轴标签、Y 轴标签，并显示图表。

```python
# 设置中文字体，使用本地电脑可用的字体
font = FontProperties(fname='/System/Library/Fonts/STHeiti Medium.ttc')

def plot_knn_results(results):
    k_values = [x['k'] for x in results]
    accuracies = [x['accuracy'] for x in results]

    plt.figure(figsize=(8, 6))
    plt.plot(k_values, accuracies, 'bo-')
    plt.xlabel('K值', fontproperties=font)
    plt.ylabel('准确率', fontproperties=font)
    plt.title('KNN 算法准确率变化', fontproperties=font)
    plt.grid(True)
    plt.show()

plot_knn_results(knn_results)
```

#### 实验结果

实验结果以表格形式呈现，表格中每一行代表一个 K 值下的实验结果，包括 K 值、准确率、训练时间和测试时间。表格按照准确率降序排列，方便我们快速找到最佳的 K 值。从表格中可以看出，当 K=3 时，KNN 算法取得了最高的准确率，为 96.32%。

```
=== K近邻分类器测试结果 ===
------------------------------------------------------------
k值     准确率        训练时间(s)      测试时间(s)     
------------------------------------------------------------
3      94.52%     0.012        5.219       
4      94.43%     0.029        5.441       
5      94.43%     0.012        5.130       
1      94.34%     0.022        5.201       
7      94.33%     0.012        5.171       
9      94.29%     0.017        5.234       
8      94.24%     0.012        5.230       
6      94.23%     0.029        5.237       
2      93.47%     0.013        5.308       
------------------------------------------------------------

最佳配置:
k值: 3
准确率: 94.52%
训练耗时: 0.012秒
预测耗时: 5.219秒
```

实验结果还以折线图的形式呈现，图表横轴表示 K 值，纵轴表示准确率。从图中可以看出，当 K=3 时，KNN 算法的准确率最高，随着 K 值的增加，准确率呈现波动下降的趋势。

![image-20241231145439457](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/image-20241231145439457.png)

## MLP 模型实现

本部分主要介绍了如何使用 Python 和 scikit-learn 库实现一个多层感知器（MLP）模型，并对模型进行训练和评估。MLP 是一种常用的深度学习模型，适用于解决各种分类和回归问题。

### 代码说明

这段代码定义了一个名为 `mlp_experiment` 的函数，该函数接收训练集数据、测试集数据、隐藏层结构列表和学习率列表作为输入，并返回一个包含所有实验结果的列表。

#### `mlp_experiment` 函数

该函数首先初始化一个空列表 `results`，用于存储实验结果。然后，使用嵌套的 `for` 循环遍历所有可能的隐藏层结构和学习率组合。对于每种组合，函数会创建一个 `MLPClassifier` 对象，并使用训练集数据对其进行训练。在训练和预测过程中，函数会记录训练时间和测试时间，并使用 `accuracy_score` 函数计算模型的准确率。最后，将所有实验结果存储在 `results` 列表中并返回。

```python
def mlp_experiment(X_train, X_test, y_train, y_test, hidden_layers, learning_rates):
    results = []
    for hidden in hidden_layers:
        for lr in learning_rates:
            print(f"正在测试 hidden_layer={hidden}, learning_rate={lr}...")
            mlp = MLPClassifier(hidden_layer_sizes=hidden,
                                learning_rate_init=lr,
                                max_iter=100)

            start_time = time.time()
            mlp.fit(X_train, y_train)
            train_time = time.time() - start_time

            start_time = time.time()
            y_pred = mlp.predict(X_test)
            test_time = time.time() - start_time

            accuracy = accuracy_score(y_test, y_pred)
            results.append({
                'hidden_layer': hidden,
                'learning_rate': lr,
                'accuracy': accuracy,
                'train_time': train_time,
                'test_time': test_time
            })
    return results
```

-   **`MLPClassifier`**: 这是 scikit-learn 库提供的多层感知器分类器类。
    -   `hidden_layer_sizes`:  指定隐藏层的结构，例如 `(500,)` 表示一个包含 500 个神经元的隐藏层。
    -   `learning_rate_init`:  指定学习率的初始值。
    -   `max_iter`:  指定最大迭代次数。
-   **`time.time()`**: 用于记录训练和测试的起始时间。
-   **`mlp.fit(X_train, y_train)`**: 使用训练数据训练模型。
-   **`mlp.predict(X_test)`**: 使用训练好的模型对测试数据进行预测。
-   **`accuracy_score(y_test, y_pred)`**: 计算模型的准确率。

#### 实验参数

代码中定义了两个列表，`hidden_layers` 和 `learning_rates`，分别用于指定实验中使用的隐藏层结构和学习率。

```python
hidden_layers = [(500,), (1000,), (1500,), (2000,)]
learning_rates = [0.001, 0.01, 0.1]
```

-   `hidden_layers`:  包含了四个不同的隐藏层结构，分别是 `(500,)`、`(1000,)`、`(1500,)` 和 `(2000,)`，表示单层隐藏层，神经元个数分别为 500, 1000, 1500 和 2000。
-   `learning_rates`:  包含了三个不同的学习率，分别是 0.001、0.01 和 0.1。

#### 模型训练

最后，代码调用 `mlp_experiment` 函数，使用训练集和测试集数据以及定义的隐藏层结构和学习率列表，训练 MLP 模型，并将所有实验结果存储在 `mlp_results` 列表中。

```python
mlp_results = mlp_experiment(X_train, X_test, y_train, y_test,
                             hidden_layers, learning_rates)
```

### 测试结果

本部分主要展示了 MLP 模型在不同隐藏层结构和学习率下的性能表现，包括分类准确率、训练时间和测试时间。为了方便分析，我们使用了表格和图表两种方式来呈现结果。

#### 测试代码说明

首先，我们来看一下用于生成测试结果的代码。代码主要分为三个部分：数据预处理、打印最佳结果和使用 Matplotlib 绘制结果图。

##### 数据预处理

这部分代码首先将 `mlp_results` 列表转换为 Pandas DataFrame 对象，方便进行数据处理和分析。然后，对训练时间和测试时间进行四舍五入，保留指定位数的小数。接着，将准确率转换为百分比形式，并对结果按照准确率进行降序排列。最后，设置 DataFrame 的列名，方便后续输出和分析。

```python
import pandas as pd

df_results = pd.DataFrame(mlp_results)

df_results['train_time'] = df_results['train_time'].round(3)
df_results['test_time'] = df_results['test_time'].round(3)
df_results['accuracy'] = (df_results['accuracy'] * 100).round(2)

df_results = df_results.sort_values('accuracy', ascending=False)

df_results.columns = ['隐藏层结构', '学习率', '准确率(%)', '训练时间(s)', '测试时间(s)']
```

-   **`pd.DataFrame(mlp_results)`**: 将 `mlp_results` 列表转换为 Pandas DataFrame 对象。
-   **`round(3)`**: 对训练时间和测试时间进行四舍五入，保留三位小数。
-   **`(df_results['accuracy'] * 100).round(2)`**: 将准确率转换为百分比形式，并保留两位小数。
-   **`sort_values('accuracy', ascending=False)`**: 按照准确率进行降序排列。
-   **`df_results.columns = [...]`**: 设置 DataFrame 的列名。

##### 打印最佳结果

这部分代码用于打印最佳的 MLP 模型结果，包括最佳隐藏层结构、最佳学习率、最佳准确率、最佳训练时间和最佳测试时间。我们首先使用 `iloc[0]` 获取排序后的 DataFrame 的第一行，即最佳结果，然后打印出来。

```python
print("--- MLP分类器实验结果 ---")
best_result = df_results.iloc[0]

print("\n=== 最佳结果 ===")
print(f"隐藏层结构: {best_result['隐藏层结构']}")
print(f"学习率: {best_result['学习率']}")
print(f"准确率: {best_result['准确率']}%")
print(f"训练时间: {best_result['训练时间(s)']}秒")
print(f"测试时间: {best_result['测试时间(s)']}秒")
```

##### 使用 Matplotlib 绘制结果图

这部分代码使用 Matplotlib 库绘制隐藏层结构、学习率和准确率之间的关系图。为了在图中正确显示中文，我们首先设置中文字体。然后，我们定义一个 `plot_mlp_results` 函数，该函数接收实验结果列表作为输入，并提取不同的隐藏层结构，然后使用 `plot` 函数绘制折线图。最后，我们设置图表的标题、X 轴标签、Y 轴标签，并显示图表。

```python
from matplotlib.font_manager import FontProperties
import matplotlib.pyplot as plt

# 设置中文字体，使用本地电脑可用的字体
font = FontProperties(fname='/System/Library/Fonts/STHeiti Medium.ttc')

def plot_mlp_results(results):
    df_results = pd.DataFrame(results)
    plt.figure(figsize=(12, 6))
    for hidden in df_results['hidden_layer'].unique():
        subset = df_results[df_results['hidden_layer'] == hidden]
        plt.plot(subset['learning_rate'], subset['accuracy'],
                 label=f'Hidden Layer: {hidden}', marker='o')

    plt.xlabel('学习率', fontproperties=font)
    plt.ylabel('准确率', fontproperties=font)
    plt.title('不同隐藏层结构和学习率下的准确率变化', fontproperties=font)
    plt.grid(True)
    plt.xscale('log')
    plt.legend(prop=font)
    plt.show()

plot_mlp_results(mlp_results)
```

-   **`FontProperties(fname='/System/Library/Fonts/STHeiti Medium.ttc')`**: 设置中文字体。
-   **`plot_mlp_results(results)`**: 绘制结果图的函数。
-   **`df_results['hidden_layer'].unique()`**: 获取所有不同的隐藏层结构。
-   **`subset = df_results[df_results['hidden_layer'] == hidden]`**: 根据隐藏层结构筛选数据。
-   **`plt.plot(subset['learning_rate'], subset['accuracy'], label=f'Hidden Layer: {hidden}', marker='o')`**: 绘制折线图。
-   **`plt.xscale('log')`**: 将 X 轴设置为对数刻度。

#### 表格结果

实验结果以表格形式呈现，表格中每一行代表一组隐藏层结构和学习率下的实验结果，包括隐藏层结构、学习率、准确率、训练时间和测试时间。表格按照准确率降序排列，方便我们快速找到最佳的模型参数。从表格中可以看出，当隐藏层结构为 `(2000,)`，学习率为 `0.01` 时，MLP 模型取得了最高的准确率，为 97.85%。

```
=== MLP分类器实验结果 ===
   隐层配置   学习率  准确率(%)  训练时间(s)  测试时间(s)
(2000,) 0.010   97.85  232.892    0.131
(1000,) 0.001   97.83   98.128    0.077
(1000,) 0.010   97.58  182.770    0.074
(1500,) 0.001   97.40  140.393    0.103
 (500,) 0.001   97.38   59.175    0.050
(2000,) 0.001   97.34  173.603    0.134
 (500,) 0.010   97.17   84.806    0.060
(1500,) 0.010   97.12  186.805    0.114
(2000,) 0.100   97.04   93.346    0.142
 (500,) 0.100   96.95   28.454    0.045
(1000,) 0.100   96.93   50.731    0.072
(1500,) 0.100   96.75   74.094    0.115

最佳配置:
隐层配置: (2000,)
学习率: 0.01
准确率: 97.85%
训练时间: 232.892秒
测试时间: 0.131秒
```

#### 图表结果

实验结果还以折线图的形式呈现，图表横轴表示学习率（对数刻度），纵轴表示准确率。每条折线代表一个不同的隐藏层结构。从图中可以看出，当学习率为 0.01 时，隐藏层结构为 (2000,) 的 MLP 模型取得了最高的准确率。

![image-20241231145621302](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/image-20241231145621302.png)

## 自定义神经网络模型实现

本部分主要介绍了如何使用 Python 和 NumPy 库实现一个简单的多层感知机（MLP）神经网络模型，并对模型进行训练和评估。该模型包含一个输入层、一个隐藏层和一个输出层，使用 Sigmoid 激活函数，并实现了前向传播、反向传播和梯度下降等核心功能。

### 代码说明

这段代码定义了一个名为 `NumpyNeuralNetwork` 的类，用于表示一个简单的多层感知机神经网络。该类包含了初始化、激活函数、前向传播、反向传播、训练和预测等方法。

#### `__init__` 方法

`__init__` 方法用于初始化神经网络的参数，包括输入层大小、隐藏层大小、输出层大小和学习率。该方法还会随机初始化权重矩阵 `W1` 和 `W2`，以及偏置向量 `b1` 和 `b2`。

```python
import numpy as np
import time
from sklearn.metrics import accuracy_score

class NumpyNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate):
        self.learning_rate = learning_rate

        # 初始化权重和偏置
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
```

-   **`input_size`**: 输入层的大小。
-   **`hidden_size`**: 隐藏层的大小。
-   **`output_size`**: 输出层的大小。
-   **`learning_rate`**: 学习率，用于控制梯度下降的速度。
-   **`np.random.randn(input_size, hidden_size) * 0.01`**: 使用标准正态分布随机初始化权重矩阵 `W1`，并乘以一个较小的系数，防止梯度爆炸。
-   **`np.zeros((1, hidden_size))`**: 初始化偏置向量 `b1` 为零向量。

#### `sigmoid` 和 `sigmoid_derivative` 方法

`sigmoid` 方法实现了 Sigmoid 激活函数，`sigmoid_derivative` 方法实现了 Sigmoid 激活函数的导数。

```python
    def sigmoid(self, x):
        x = np.clip(x, -500, 500)   # 防止溢出
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)
```

-   **`np.clip(x, -500, 500)`**: 将输入值限制在 -500 到 500 之间，防止 `np.exp(-x)` 出现溢出。
-   **`1 / (1 + np.exp(-x))`**: Sigmoid 函数的定义。
-   **`x * (1 - x)`**: Sigmoid 函数的导数。

#### `softmax` 方法

`softmax` 方法实现了 Softmax 激活函数，用于多分类问题。

```python
    def softmax(self, x):
      exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
      return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

-   **`np.exp(x - np.max(x, axis=1, keepdims=True))`**: 计算指数值，并减去最大值，防止溢出。
-   **`exp_x / np.sum(exp_x, axis=1, keepdims=True)`**: Softmax 函数的定义。

#### `forward` 方法

`forward` 方法实现了神经网络的前向传播过程，包括计算隐藏层输出和输出层输出。

```python
    def forward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.softmax(self.z2)
        return self.a2
```

-   **`np.dot(X, self.W1) + self.b1`**: 计算隐藏层的线性输出。
-   **`self.sigmoid(self.z1)`**: 使用 Sigmoid 激活函数计算隐藏层的激活输出。
-   **`np.dot(self.a1, self.W2) + self.b2`**: 计算输出层的线性输出。
-   **`self.softmax(self.z2)`**: 使用 Softmax 激活函数计算输出层的激活输出。

#### `backward` 方法

`backward` 方法实现了神经网络的反向传播过程，包括计算输出层和隐藏层的误差，并更新权重和偏置。

```python
    def backward(self, X, y, output):
        self.output_error = output - y

        self.z2_error = self.output_error
        self.z2_delta = self.z2_error * self.sigmoid_derivative(output)

        self.W2 -= self.learning_rate * np.dot(self.a1.T, self.z2_delta)
        self.b2 -= self.learning_rate * np.sum(self.z2_delta, axis=0, keepdims=True)

        self.z1_error = np.dot(self.z2_delta, self.W2.T)
        self.z1_delta = self.z1_error * self.sigmoid_derivative(self.a1)

        self.W1 -= self.learning_rate * np.dot(X.T, self.z1_delta)
        self.b1 -= self.learning_rate * np.sum(self.z1_delta, axis=0, keepdims=True)
```

-   **`output - y`**: 计算输出层的误差。
-   **`self.z2_error * self.sigmoid_derivative(output)`**: 计算输出层的梯度。
-   **`self.learning_rate * np.dot(self.a1.T, self.z2_delta)`**: 更新权重矩阵 `W2`。
-   **`self.learning_rate * np.sum(self.z2_delta, axis=0, keepdims=True)`**: 更新偏置向量 `b2`。
-   **`np.dot(self.z2_delta, self.W2.T)`**: 计算隐藏层的误差。
-   **`self.z1_error * self.sigmoid_derivative(self.a1)`**: 计算隐藏层的梯度。
-   **`self.learning_rate * np.dot(X.T, self.z1_delta)`**: 更新权重矩阵 `W1`。
-   **`self.learning_rate * np.sum(self.z1_delta, axis=0, keepdims=True)`**: 更新偏置向量 `b1`。

#### `train` 方法

`train` 方法实现了神经网络的训练过程，包括前向传播、反向传播和损失计算。

```python
    def train(self, X, y, epochs):
        for epoch in range(epochs):
            output = self.forward(X)
            self.backward(X, y, output)
            if epoch % 100 == 0:
                loss = -np.mean(np.sum(y * np.log(output + 1e-8), axis=1))
                print(f'Epoch {epoch}, Loss: {loss}')
```

-   **`epochs`**: 训练的轮数。
-   **`self.forward(X)`**: 进行前向传播。
-   **`self.backward(X, y, output)`**: 进行反向传播。
-   **`-np.mean(np.sum(y * np.log(output + 1e-8), axis=1))`**: 计算交叉熵损失。

#### `predict` 方法

`predict` 方法实现了神经网络的预测过程，即对输入数据进行前向传播，并返回预测结果。

```python
    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)
```

-   **`np.argmax(output, axis=1)`**: 返回输出层中概率最大的类别索引。

#### `numpy_nn_experiment` 函数

`numpy_nn_experiment` 函数用于进行神经网络的实验，包括数据预处理、模型训练和模型评估。

```python
def numpy_nn_experiment(X_train, X_test, y_train, y_test, hidden_sizes, learning_rates):
    results = []
    # 将标签转换为 one-hot 编码
    y_train_onehot = np.zeros((y_train.shape[0], 10))
    for i, label in enumerate(y_train):
        y_train_onehot[i, int(label)] = 1

    for hidden_size in hidden_sizes:
        for lr in learning_rates:
            print(f"正在测试 hidden size: {hidden_size}, learning rate: {lr}...")
            nn = NumpyNeuralNetwork(784, hidden_size, 10, lr)

            start_time = time.time()
            nn.train(X_train, y_train_onehot, epochs=500)
            train_time = time.time() - start_time

            start_time = time.time()
            y_pred = nn.predict(X_test)
            test_time = time.time() - start_time

            accuracy = accuracy_score(y_test.astype(int), y_pred)
            results.append({
                'hidden_size': hidden_size,
                'learning_rate': lr,
                'accuracy': accuracy,
                'train_time': train_time,
                'test_time': test_time
            })
    return results
```

-   **`y_train_onehot = np.zeros((y_train.shape[0], 10))`**: 将训练集标签转换为 one-hot 编码。
-   **`NumpyNeuralNetwork(784, hidden_size, 10, lr)`**: 创建一个 `NumpyNeuralNetwork` 对象。
-   **`nn.train(X_train, y_train_onehot, epochs=500)`**: 训练模型。
-   **`nn.predict(X_test)`**: 使用训练好的模型对测试数据进行预测。
-   **`accuracy_score(y_test.astype(int), y_pred)`**: 计算模型的准确率。

#### 实验参数

代码中定义了隐藏层大小列表 `hidden_layers` 和学习率列表 `learning_rates`，用于指定实验中使用的参数。

```python
hidden_layers = [100, 200, 300, 400, 500]
learning_rates = [0.001, 0.01, 0.1]
```

-   **`hidden_layers`**: 包含了不同的隐藏层大小。
-   **`learning_rates`**: 包含了不同的学习率。

#### 模型训练

最后，代码调用 `numpy_nn_experiment` 函数，使用训练集和测试集数据以及定义的隐藏层大小和学习率列表，训练神经网络模型，并将所有实验结果存储在 `numpy_nn_results` 列表中。

```python
numpy_nn_results = numpy_nn_experiment(X_train, X_test, y_train, y_test, hidden_layers, learning_rates)
```

### 测试结果

本部分主要介绍了如何对自定义神经网络模型的实验结果进行分析和可视化。通过表格和图表的形式，展示了不同隐藏层大小和学习率对模型性能的影响，并总结了最佳的参数配置。

```python
# 设置中文字体，使用苹果电脑自带的字体
font = FontProperties(fname='/System/Library/Fonts/STHeiti Medium.ttc')

# 创建DataFrame并展示
df_results = pd.DataFrame(numpy_nn_results)

# 格式化时间和准确率(转换为百分比)
df_results['train_time'] = df_results['train_time'].round(3)
df_results['test_time'] = df_results['test_time'].round(3) 
df_results['accuracy'] = (df_results['accuracy'] * 100).round(2)

# 按准确率降序排序
df_results = df_results.sort_values('accuracy', ascending=False)

# 设置列名的中文显示 
df_results.columns = ['隐层大小', '学习率', '准确率(%)', '训练时间(s)', '测试时间(s)']

# 打印结果
print("\n=== Numpy神经网络实验结果 ===")
print("-" * 60)
print(df_results.to_string(index=False))
print("-" * 60)

# 打印最佳配置
best_result = df_results.iloc[0]
print(f"\n最佳配置:")
print(f"隐层大小: {best_result['隐层大小']}")  
print(f"学习率: {best_result['学习率']}")
print(f"准确率: {best_result['准确率(%)']}%")
print(f"训练耗时: {best_result['训练时间(s)']}秒")
print(f"预测耗时: {best_result['测试时间(s)']}秒")

# 可视化结果
plt.figure(figsize=(12, 6))

# 获取所有唯一的隐层大小
hidden_sizes = df_results['隐层大小'].unique()
for hidden in hidden_sizes:
    # 获取当前隐层大小的数据并按学习率排序
    subset = df_results[df_results['隐层大小'] == hidden].sort_values('学习率')
    plt.plot(subset['学习率'], subset['准确率(%)'], 
            label=f'隐层大小: {hidden}', marker='o')

plt.xlabel('学习率', fontproperties=font)
plt.ylabel('准确率', fontproperties=font)
plt.title('Numpy神经网络: 学习率与准确率的关系', fontproperties=font)
plt.legend(prop=font)
plt.grid(True)
plt.xscale('log')
plt.show()
```

代码的输出包括：

1.  **实验结果表格**: 展示了不同参数组合下的准确率、训练时间和测试时间。
2.  **最佳配置**: 指出了准确率最高的参数组合。
3.  **折线图**: 展示了不同隐藏层大小下，准确率随学习率的变化趋势。

#### 表格结果

这份表格展示了使用不同超参数配置训练的 Numpy 神经网络的实验结果，包括隐藏层大小、学习率、准确率、训练时间和测试时间。最佳配置为：隐藏层大小 400，学习率 0.01，准确率 93.91%，训练耗时 632.45 秒，预测耗时 0.123 秒。

```
=== Numpy神经网络实验结果 ===
------------------------------------------------------------
 隐层大小   学习率  准确率(%)  训练时间(s)  测试时间(s)
  400 0.010   93.91  632.450    0.123
  300 0.010   93.85  522.945    0.114
  500 0.001   93.53  816.090    0.172
  400 0.001   93.27  639.106    0.131
  200 0.001   93.23  375.813    0.082
  500 0.100   93.00  825.192    0.206
  300 0.001   92.73  515.987    0.104
  200 0.010   92.68  383.980    0.085
  300 0.100   92.64  533.261    0.109
  100 0.010   92.33  283.933    0.076
  100 0.001   91.97  287.192    0.051
  200 0.100   91.27  386.116    0.082
  500 0.010   90.39  817.261    0.179
  100 0.100   88.70  261.872    0.045
  400 0.100   83.00  646.657    0.140
------------------------------------------------------------

最佳配置:
隐层大小: 400.0
学习率: 0.01
准确率: 93.91%
训练耗时: 632.45秒
预测耗时: 0.123秒
```

#### 图表结果

下图展示了不同隐藏层大小下，模型准确率随学习率变化的趋势。横轴表示学习率，使用对数刻度，纵轴表示准确率。每条线代表一个特定的隐藏层大小，并用不同的颜色区分。综合来看，隐藏层大小为 400，学习率为 0.01 时，模型表现最佳。该图直观地展示了不同参数对模型性能的影响，为选择合适的超参数提供了依据。

![image-20241231145756841](https://ceyewan.oss-cn-beijing.aliyuncs.com/typora/image-20241231145756841.png)

## 小结

本实验报告详细介绍了手写数字识别任务中，KNN、多层感知机（MLP）以及自定义Numpy神经网络的实现与性能比较。首先，我们加载并预处理了MNIST数据集，然后分别实现了KNN和MLP模型，并使用不同的超参数组合进行了实验。实验结果表明，对于KNN算法，当K=3时准确率最高，达到94.52%；对于MLP模型，隐藏层结构为(2000,)、学习率为0.01时取得最佳效果，准确率高达97.85%。此外，我们还自行实现了基于Numpy的神经网络，并进行了实验。实验结果表明，不同的超参数配置对模型性能有显著影响。通过对比分析，我们发现MLP模型在手写数字识别任务中表现更优，但训练时间较长。本实验通过表格和图表清晰展示了不同模型的性能特点，为后续模型优化提供了参考。
